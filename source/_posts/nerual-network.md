title: nerual_network简单介绍
date: 2016-02-23 19:42:47
tags: "machine learning"
---
## 感知机
首先说一下感知机，感知机长怎么样
如图：
![201602231](https://raw.githubusercontent.com/xsj4cs/xsj4cs.github.io/hexo/images/201602231.png)
左边三个是输入，然后这三个输入分别乘以自己相对应的系数wi，然后相加，如果值大于某个自己设定的阈值，那么输出1，小于则输出0。
## 神经网络
神经网络其实和感知机差不多，只不过一般是好几层，而且一般输出也不是0或1，而是一个实数，而且为了能够更加精确，且能使用梯度下降（什么是梯度下降等下讲），一般对于输入的数据与对应系数wi相乘后，相加，然后减去对应阈值（这里变为+b，好看）后得到的值经过过一个可微的激发函数，常用sigmoid函数f(x)=(1-e^-x)/(1+e^-x)。看下图的神经网络图：
![201602232](https://raw.githubusercontent.com/xsj4cs/xsj4cs.github.io/hexo/images/201602232.png)
输入层，输入数据经过中间的隐层，输出到最后的输出层，中间的隐藏和输出层，都和上面的感知机一样，对于所有输入乘以对应的系数，然后相加，加上阈值b，带入激发函数，把结果作为下层的输入。和其他机器学习算法，如svn，贝叶斯一样，其实就是要求解wi，使得输出更加准确，误差就是e=Σ(xim - yi)^2,最后一次与实际结果的平方差。
## 梯度下降算法
再说怎么解神经网络的系数前，先说一下什么是梯度下降算法。梯度算法，其实就是为了使得误差e变得更小，那么就要对误差e对各系数求偏导，这样这些值就组成了梯度（高等数学中应该是有这个概念的，倒三角），也就是可以认为是一个方向，然后系数往这个方向移动某段很小的距离将逼近极小值，那么就设定一个步长η，那么不对迭代，可以使得e达到接近极小值，那么也就是我们要的wi系数。
## bp算法
说完了梯度下降，那么我们说一下如何解神经网络的系数，这里介绍一种经典的方法，bp算法，其实很简单，就是从输出层，往前不断的做梯度下降，然后在根据这些新得到的系数，求得新的误差e，然后继续往前，这样不断迭代即可。
## 梯度下降和bp的推导
![201602233](https://raw.githubusercontent.com/xsj4cs/xsj4cs.github.io/hexo/images/201602233.png)
![201602234](https://raw.githubusercontent.com/xsj4cs/xsj4cs.github.io/hexo/images/201602234.png)
![201602235](https://raw.githubusercontent.com/xsj4cs/xsj4cs.github.io/hexo/images/201602235.png)
![201602236](https://raw.githubusercontent.com/xsj4cs/xsj4cs.github.io/hexo/images/201602236.png)
## 怎么用
以简单的例子来说明：
我们需要一个算法来识别手写的数字，那么如何识别呢？
首先，要有输入，输入是什么，这里我们把图片的每一个像素点作为输入（假设只有黑白)，那么黑的为1，白的为0。这样就行了
